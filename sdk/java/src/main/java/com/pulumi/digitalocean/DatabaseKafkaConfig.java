// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.digitalocean;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.digitalocean.DatabaseKafkaConfigArgs;
import com.pulumi.digitalocean.Utilities;
import com.pulumi.digitalocean.inputs.DatabaseKafkaConfigState;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import javax.annotation.Nullable;

/**
 * Provides a virtual resource that can be used to change advanced configuration
 * options for a DigitalOcean managed Kafka database cluster.
 * 
 * &gt; **Note** Kafka configurations are only removed from state when destroyed. The remote configuration is not unset.
 * 
 * ## Example Usage
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.digitalocean.DatabaseCluster;
 * import com.pulumi.digitalocean.DatabaseClusterArgs;
 * import com.pulumi.digitalocean.DatabaseKafkaConfig;
 * import com.pulumi.digitalocean.DatabaseKafkaConfigArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var exampleDatabaseCluster = new DatabaseCluster("exampleDatabaseCluster", DatabaseClusterArgs.builder()
 *             .name("example-kafka-cluster")
 *             .engine("kafka")
 *             .version("3.7")
 *             .size("db-s-1vcpu-1gb")
 *             .region("nyc3")
 *             .nodeCount(3)
 *             .build());
 * 
 *         var example = new DatabaseKafkaConfig("example", DatabaseKafkaConfigArgs.builder()
 *             .clusterId(exampleDatabaseCluster.id())
 *             .groupInitialRebalanceDelayMs(3000)
 *             .groupMinSessionTimeoutMs(6000)
 *             .groupMaxSessionTimeoutMs(1800000)
 *             .messageMaxBytes(1048588)
 *             .logCleanerDeleteRetentionMs(86400000)
 *             .logCleanerMinCompactionLagMs("0")
 *             .logFlushIntervalMs("9223372036854775807")
 *             .logIndexIntervalBytes(4096)
 *             .logMessageDownconversionEnable(true)
 *             .logMessageTimestampDifferenceMaxMs("9223372036854775807")
 *             .logPreallocate(false)
 *             .logRetentionBytes("-1")
 *             .logRetentionHours(168)
 *             .logRetentionMs("604800000")
 *             .logRollJitterMs("0")
 *             .logSegmentDeleteDelayMs(60000)
 *             .autoCreateTopicsEnable(true)
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ## Import
 * 
 * A Kafka database cluster&#39;s configuration can be imported using the `id` the parent cluster, e.g.
 * 
 * ```sh
 * $ pulumi import digitalocean:index/databaseKafkaConfig:DatabaseKafkaConfig example 4b62829a-9c42-465b-aaa3-84051048e712
 * ```
 * 
 */
@ResourceType(type="digitalocean:index/databaseKafkaConfig:DatabaseKafkaConfig")
public class DatabaseKafkaConfig extends com.pulumi.resources.CustomResource {
    /**
     * Enable auto creation of topics.
     * 
     */
    @Export(name="autoCreateTopicsEnable", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> autoCreateTopicsEnable;

    /**
     * @return Enable auto creation of topics.
     * 
     */
    public Output<Boolean> autoCreateTopicsEnable() {
        return this.autoCreateTopicsEnable;
    }
    /**
     * The ID of the target Kafka cluster.
     * 
     */
    @Export(name="clusterId", refs={String.class}, tree="[0]")
    private Output<String> clusterId;

    /**
     * @return The ID of the target Kafka cluster.
     * 
     */
    public Output<String> clusterId() {
        return this.clusterId;
    }
    /**
     * The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.
     * 
     */
    @Export(name="groupInitialRebalanceDelayMs", refs={Integer.class}, tree="[0]")
    private Output<Integer> groupInitialRebalanceDelayMs;

    /**
     * @return The amount of time, in milliseconds, the group coordinator will wait for more consumers to join a new group before performing the first rebalance. A longer delay means potentially fewer rebalances, but increases the time until processing begins. The default value for this is 3 seconds. During development and testing it might be desirable to set this to 0 in order to not delay test execution time.
     * 
     */
    public Output<Integer> groupInitialRebalanceDelayMs() {
        return this.groupInitialRebalanceDelayMs;
    }
    /**
     * The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
     * 
     */
    @Export(name="groupMaxSessionTimeoutMs", refs={Integer.class}, tree="[0]")
    private Output<Integer> groupMaxSessionTimeoutMs;

    /**
     * @return The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
     * 
     */
    public Output<Integer> groupMaxSessionTimeoutMs() {
        return this.groupMaxSessionTimeoutMs;
    }
    /**
     * The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
     * 
     */
    @Export(name="groupMinSessionTimeoutMs", refs={Integer.class}, tree="[0]")
    private Output<Integer> groupMinSessionTimeoutMs;

    /**
     * @return The minimum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures.
     * 
     */
    public Output<Integer> groupMinSessionTimeoutMs() {
        return this.groupMinSessionTimeoutMs;
    }
    /**
     * How long are delete records retained?
     * 
     */
    @Export(name="logCleanerDeleteRetentionMs", refs={Integer.class}, tree="[0]")
    private Output<Integer> logCleanerDeleteRetentionMs;

    /**
     * @return How long are delete records retained?
     * 
     */
    public Output<Integer> logCleanerDeleteRetentionMs() {
        return this.logCleanerDeleteRetentionMs;
    }
    /**
     * The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.
     * 
     */
    @Export(name="logCleanerMinCompactionLagMs", refs={String.class}, tree="[0]")
    private Output<String> logCleanerMinCompactionLagMs;

    /**
     * @return The minimum time a message will remain uncompacted in the log. Only applicable for logs that are being compacted.
     * 
     */
    public Output<String> logCleanerMinCompactionLagMs() {
        return this.logCleanerMinCompactionLagMs;
    }
    /**
     * The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used.
     * 
     */
    @Export(name="logFlushIntervalMs", refs={String.class}, tree="[0]")
    private Output<String> logFlushIntervalMs;

    /**
     * @return The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used.
     * 
     */
    public Output<String> logFlushIntervalMs() {
        return this.logFlushIntervalMs;
    }
    /**
     * The interval with which Kafka adds an entry to the offset index.
     * 
     */
    @Export(name="logIndexIntervalBytes", refs={Integer.class}, tree="[0]")
    private Output<Integer> logIndexIntervalBytes;

    /**
     * @return The interval with which Kafka adds an entry to the offset index.
     * 
     */
    public Output<Integer> logIndexIntervalBytes() {
        return this.logIndexIntervalBytes;
    }
    /**
     * This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests.
     * 
     */
    @Export(name="logMessageDownconversionEnable", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> logMessageDownconversionEnable;

    /**
     * @return This configuration controls whether down-conversion of message formats is enabled to satisfy consume requests.
     * 
     */
    public Output<Boolean> logMessageDownconversionEnable() {
        return this.logMessageDownconversionEnable;
    }
    /**
     * The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message.
     * 
     */
    @Export(name="logMessageTimestampDifferenceMaxMs", refs={String.class}, tree="[0]")
    private Output<String> logMessageTimestampDifferenceMaxMs;

    /**
     * @return The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message.
     * 
     */
    public Output<String> logMessageTimestampDifferenceMaxMs() {
        return this.logMessageTimestampDifferenceMaxMs;
    }
    /**
     * Controls whether to preallocate a file when creating a new segment.
     * 
     */
    @Export(name="logPreallocate", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> logPreallocate;

    /**
     * @return Controls whether to preallocate a file when creating a new segment.
     * 
     */
    public Output<Boolean> logPreallocate() {
        return this.logPreallocate;
    }
    /**
     * The maximum size of the log before deleting messages.
     * 
     */
    @Export(name="logRetentionBytes", refs={String.class}, tree="[0]")
    private Output<String> logRetentionBytes;

    /**
     * @return The maximum size of the log before deleting messages.
     * 
     */
    public Output<String> logRetentionBytes() {
        return this.logRetentionBytes;
    }
    /**
     * The number of hours to keep a log file before deleting it.
     * 
     */
    @Export(name="logRetentionHours", refs={Integer.class}, tree="[0]")
    private Output<Integer> logRetentionHours;

    /**
     * @return The number of hours to keep a log file before deleting it.
     * 
     */
    public Output<Integer> logRetentionHours() {
        return this.logRetentionHours;
    }
    /**
     * The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
     * 
     */
    @Export(name="logRetentionMs", refs={String.class}, tree="[0]")
    private Output<String> logRetentionMs;

    /**
     * @return The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
     * 
     */
    public Output<String> logRetentionMs() {
        return this.logRetentionMs;
    }
    /**
     * The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used.
     * 
     */
    @Export(name="logRollJitterMs", refs={String.class}, tree="[0]")
    private Output<String> logRollJitterMs;

    /**
     * @return The maximum jitter to subtract from logRollTimeMillis (in milliseconds). If not set, the value in log.roll.jitter.hours is used.
     * 
     */
    public Output<String> logRollJitterMs() {
        return this.logRollJitterMs;
    }
    /**
     * The amount of time to wait before deleting a file from the filesystem.
     * 
     */
    @Export(name="logSegmentDeleteDelayMs", refs={Integer.class}, tree="[0]")
    private Output<Integer> logSegmentDeleteDelayMs;

    /**
     * @return The amount of time to wait before deleting a file from the filesystem.
     * 
     */
    public Output<Integer> logSegmentDeleteDelayMs() {
        return this.logSegmentDeleteDelayMs;
    }
    /**
     * The maximum size of message that the server can receive.
     * 
     */
    @Export(name="messageMaxBytes", refs={Integer.class}, tree="[0]")
    private Output<Integer> messageMaxBytes;

    /**
     * @return The maximum size of message that the server can receive.
     * 
     */
    public Output<Integer> messageMaxBytes() {
        return this.messageMaxBytes;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public DatabaseKafkaConfig(java.lang.String name) {
        this(name, DatabaseKafkaConfigArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public DatabaseKafkaConfig(java.lang.String name, DatabaseKafkaConfigArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public DatabaseKafkaConfig(java.lang.String name, DatabaseKafkaConfigArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("digitalocean:index/databaseKafkaConfig:DatabaseKafkaConfig", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private DatabaseKafkaConfig(java.lang.String name, Output<java.lang.String> id, @Nullable DatabaseKafkaConfigState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("digitalocean:index/databaseKafkaConfig:DatabaseKafkaConfig", name, state, makeResourceOptions(options, id), false);
    }

    private static DatabaseKafkaConfigArgs makeArgs(DatabaseKafkaConfigArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? DatabaseKafkaConfigArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static DatabaseKafkaConfig get(java.lang.String name, Output<java.lang.String> id, @Nullable DatabaseKafkaConfigState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new DatabaseKafkaConfig(name, id, state, options);
    }
}
